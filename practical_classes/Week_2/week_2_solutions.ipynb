{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages we will need for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n",
    "                                    'sample_audvis_filt-0-40_raw.fif')\n",
    "raw = mne.io.read_raw_fif(sample_data_raw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Note that the data we are loading is filtered from 0.1 to 40 Hz so we do not need to filter it (for now).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set up and fit the ICA\n",
    "ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "ica.fit(raw)\n",
    "ica.exclude = [1, 2]  # details on how we picked these are omitted here\n",
    "ica.plot_properties(raw, picks=ica.exclude);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "orig_raw = raw.copy()\n",
    "raw.load_data()\n",
    "ica.apply(raw)\n",
    "\n",
    "# show some frontal channels to clearly illustrate the artifact removal\n",
    "chs = ['MEG 0111', 'MEG 0121', 'MEG 0131', 'MEG 0211', 'MEG 0221', 'MEG 0231',\n",
    "       'MEG 0311', 'MEG 0321', 'MEG 0331', 'MEG 1511', 'MEG 1521', 'MEG 1531',\n",
    "       'EEG 001', 'EEG 002', 'EEG 003', 'EEG 004', 'EEG 005', 'EEG 006',\n",
    "       'EEG 007', 'EEG 008']\n",
    "chan_idxs = [raw.ch_names.index(ch) for ch in chs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Detecting experimental events\n",
    "\n",
    "The sample dataset includes several :term:`\"STIM\" channels <stim channel>`\n",
    "that recorded electrical\n",
    "signals sent from the stimulus delivery computer (as brief DC shifts /\n",
    "squarewave pulses). These pulses (often called \"triggers\") are used in this\n",
    "dataset to mark experimental events: stimulus onset, stimulus type, and\n",
    "participant response (button press). The individual STIM channels are\n",
    "combined onto a single channel, in such a way that voltage\n",
    "levels on that channel can be unambiguously decoded as a particular event\n",
    "type. On older Neuromag systems (such as that used to record the sample data)\n",
    "this summation channel was called ``STI 014``, so we can pass that channel\n",
    "name to the :func:`mne.find_events` function to recover the timing and\n",
    "identity of the stimulus events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "events = mne.find_events(raw, stim_channel='STI 014')\n",
    "print(events[:5])  # show the first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n",
    "              'visual/right': 4, 'smiley': 5, 'buttonpress': 32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For paradigms that are not event-related (e.g., analysis of resting-state\n",
    "data), you can extract regularly spaced (possibly overlapping) spans of data\n",
    "by creating events using :func:`mne.make_fixed_length_events` and then\n",
    "proceeding with epoching as described in the next section.\n",
    "\n",
    "\n",
    "\n",
    "## Epoching continuous data\n",
    "\n",
    "The :class:`~mne.io.Raw` object and the events array are the bare minimum\n",
    "needed to create an :class:`~mne.Epochs` object, which we create with the\n",
    ":class:`~mne.Epochs` class constructor. Here we'll also specify some data\n",
    "quality constraints: we'll reject any epoch where peak-to-peak signal\n",
    "amplitude is beyond reasonable limits for that channel type. This is done\n",
    "with a *rejection dictionary*; you may include or omit thresholds for any of\n",
    "the channel types present in your data. The values given here are reasonable\n",
    "for this particular dataset, but may need to be adapted for different\n",
    "hardware or recording conditions. For a more automated approach, consider\n",
    "using the `autoreject package`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "                       grad=4000e-13,    # 4000 fT/cm\n",
    "                       eeg=150e-6,       # 150 µV\n",
    "                       eog=250e-6)       # 250 µV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also pass the event dictionary as the ``event_id`` parameter (so we can\n",
    "work with easy-to-pool event labels instead of the integer event IDs), and\n",
    "specify ``tmin`` and ``tmax`` (the time relative to each event at which to\n",
    "start and end each epoch). As mentioned above, by default\n",
    ":class:`~mne.io.Raw` and :class:`~mne.Epochs` data aren't loaded into memory\n",
    "(they're accessed from disk only when needed), but here we'll force loading\n",
    "into memory using the ``preload=True`` parameter so that we can see the\n",
    "results of the rejection criteria being applied:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5,\n",
    "                    reject=reject_criteria, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in the EEG channels, we will select them. \n",
    "__Note__ the command works \"in-place\", i.e. it changes the object in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.pick_types(meg=False, eeg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll pool across left/right stimulus presentations so we can compare\n",
    "auditory versus visual responses. To avoid biasing our signals to the\n",
    "left or right, we'll use :meth:`~mne.Epochs.equalize_event_counts` first to\n",
    "randomly sample epochs from each condition to match the number of epochs\n",
    "present in the condition with the fewest good epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "conds_we_care_about = ['auditory/left', 'auditory/right',\n",
    "                       'visual/left', 'visual/right']\n",
    "epochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\n",
    "aud_epochs = epochs['auditory']\n",
    "vis_epochs = epochs['visual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like :class:`~mne.io.Raw` objects, :class:`~mne.Epochs` objects also have a\n",
    "number of built-in plotting methods. One is :meth:`~mne.Epochs.plot_image`,\n",
    "which shows each epoch as one row of an image map, with color representing\n",
    "signal magnitude; the average evoked response and the sensor location are\n",
    "shown below the image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evoked plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_left_evoked = aud_epochs['left'].average()\n",
    "aud_right_evoked = aud_epochs['right'].average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_left_evoked.plot_joint();\n",
    "aud_right_evoked.plot_joint();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a \"difference\" object be combining the _left_ and _right_ stimuli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_evoked = mne.combine_evoked([aud_left_evoked, -aud_right_evoked], weights='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_evoked.plot_joint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the peak changed by a lower low pass setting, e.g. 30 Hz?\n",
    "\n",
    "To test this we first need to find the peak for both and then compare it.\n",
    "We can use the \"get_peak\" method for that. Remember that help and default values can be found with \"?\", e.g.: \n",
    "\n",
    "```aud_left_evoked.get_peak?```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_left_evoked.get_peak(return_amplitude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_right_evoked.get_peak(return_amplitude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compare to the different low pass we can filter the epochs. \n",
    "\n",
    "__NOTE__ This can potentially lead to massive artifacts and should __NOT__ be done. But since it is just a quick excerise we will still do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_30 = epochs.copy()  # make a copy to not overwrite the origianls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_30.filter(0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to show it, we can chain a lot of the commands together. It is better to be explicit with your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_30['auditory/left'].average().get_peak(return_amplitude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_30['auditory/right'].average().get_peak(return_amplitude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are excatly identical to the orignals, so it does not appear that the low pass changes the peak time. However the amplitute is not the same, __why__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global field power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_left_data = aud_epochs['left'].get_data()\n",
    "aud_right_data = aud_epochs['right'].get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined GFP as the standard deviation of the evoked signal over time, so if we take the ```np.std``` of the evoked data we get the GFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_gfp = aud_left_evoked._data.std(axis=0)\n",
    "right_gfp = aud_right_evoked._data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to the times of the samples to make a useful plot, we can get those from any of the object with ```.times```, e.g. ```aud_left_evoked.times```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = aud_left_evoked.times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, left_gfp, 'r', label='Left');  # the 'r' makes the line red\n",
    "plt.plot(times, right_gfp, 'b', label='right');  # label is what goes in the legend\n",
    "plt.grid();\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import spatio_temporal_cluster_test\n",
    "from mne.channels import find_ch_adjacency\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permutation test functions in MNE-python does not work on epochs or evoked directly so we have to extract the data we want to use first. We will call the data tensor for __X__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [aud_epochs[condition].get_data() for condition in ['left', 'right']]  # as 3D matrix\n",
    "X = [np.transpose(x, (0, 2, 1)) for x in X]  # transpose for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to let the algorithm know which sensors are neighbors we first generate an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency, ch_names = find_ch_adjacency(epochs.info, ch_type='eeg')\n",
    "\n",
    "print(type(adjacency))  # it's a sparse matrix!\n",
    "\n",
    "plt.imshow(adjacency.toarray(), cmap='binary', origin='lower',\n",
    "           interpolation='nearest')\n",
    "plt.xlabel('{} EEG channels'.format(len(ch_names)))\n",
    "plt.ylabel('{} EEG channels '.format(len(ch_names)))\n",
    "plt.grid();\n",
    "plt.title('Between-sensor adjacency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cluster threshold\n",
    "# set family-wise p-value\n",
    "p_accept = 0.01\n",
    "\n",
    "cluster_stats = spatio_temporal_cluster_test(X,\n",
    "                                             n_permutations=5000,\n",
    "                                             threshold=None,\n",
    "                                             tail=0,\n",
    "                                             n_jobs=1,\n",
    "                                             buffer_size=None,\n",
    "                                             adjacency=adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs, clusters, p_values, h_zero = cluster_stats\n",
    "good_cluster_inds = np.where(p_values < p_accept)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to visualize the results. There is a lot going on in the code so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure variables for visualization\n",
    "colors = {\"left\": \"crimson\", \"right\": 'steelblue'}\n",
    "\n",
    "# organize data for plotting\n",
    "evokeds = {cond: epochs[cond].average() for cond in ['left', 'right']}\n",
    "\n",
    "# loop over clusters\n",
    "for i_clu, clu_idx in enumerate(good_cluster_inds):\n",
    "    # unpack cluster information, get unique indices\n",
    "    time_inds, space_inds = np.squeeze(clusters[clu_idx])\n",
    "    ch_inds = np.unique(space_inds)\n",
    "    time_inds = np.unique(time_inds)\n",
    "\n",
    "    # get topography for F stat\n",
    "    f_map = T_obs[time_inds, ...].mean(axis=0)\n",
    "\n",
    "    # get signals at the sensors contributing to the cluster\n",
    "    sig_times = epochs.times[time_inds]\n",
    "\n",
    "    # create spatial mask\n",
    "    mask = np.zeros((f_map.shape[0], 1), dtype=bool)\n",
    "    mask[ch_inds, :] = True\n",
    "\n",
    "    # initialize figure\n",
    "    fig, ax_topo = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "    # plot average test statistic and mark significant sensors\n",
    "    f_evoked = mne.EvokedArray(f_map[:, np.newaxis], epochs.info, tmin=0)\n",
    "    f_evoked.plot_topomap(times=0, mask=mask, axes=ax_topo, cmap='Reds',\n",
    "                          vmin=np.min, vmax=np.max, show=False,\n",
    "                          colorbar=False, mask_params=dict(markersize=10))\n",
    "    image = ax_topo.images[0]\n",
    "\n",
    "    # create additional axes (for ERF and colorbar)\n",
    "    divider = make_axes_locatable(ax_topo)\n",
    "\n",
    "    # add axes for colorbar\n",
    "    ax_colorbar = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    plt.colorbar(image, cax=ax_colorbar)\n",
    "    ax_topo.set_xlabel(\n",
    "        'Averaged F-map ({:0.3f} - {:0.3f} s)'.format(*sig_times[[0, -1]]))\n",
    "\n",
    "    # add new axis for time courses and plot time courses\n",
    "    ax_signals = divider.append_axes('right', size='300%', pad=1.2)\n",
    "    title = 'Cluster #{0}, {1} sensor'.format(i_clu + 1, len(ch_inds))\n",
    "    if len(ch_inds) > 1:\n",
    "        title += \"s (mean)\"\n",
    "    mne.viz.plot_compare_evokeds(evokeds, title=title, picks=ch_inds, axes=ax_signals,\n",
    "                         colors=colors, show=False,\n",
    "                         split_legend=True, truncate_yaxis='auto')\n",
    "\n",
    "    # plot temporal cluster extent\n",
    "    ymin, ymax = ax_signals.get_ylim()\n",
    "    ax_signals.fill_betweenx((ymin, ymax), sig_times[0], sig_times[-1],\n",
    "                             color='orange', alpha=0.3)\n",
    "\n",
    "    # clean up viz\n",
    "    mne.viz.tight_layout(fig=fig)\n",
    "    fig.subplots_adjust(bottom=.05)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
