{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book is based on 3.cnn.for.digits.classification.ipynb from https://github.com/PacktPublishing/Deep-learning-with-PyTorch-video by Anand Saha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpnZMQZqHmKK"
   },
   "source": [
    "### ConvNet for digits recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_Sukk0UHmKK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Torchvision module contains various utilities, classes, models and datasets \n",
    "# used towards computer vision usecases\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# Functional module contains helper functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AILF61e9HmKL"
   },
   "source": [
    "**Create the DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp1Ef4oDHmKL"
   },
   "outputs": [],
   "source": [
    "# Mean and standard deviation of all the pixels in the MNIST dataset\n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
    "\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUKBjruwHmKL"
   },
   "outputs": [],
   "source": [
    "img = mnist_train[12][0].numpy() * stddev_gray + mean_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xMiviIbHmKL"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "# Note that each image is 28 x 28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Sc3EgBtHmKM"
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16m4jgjFHmKM"
   },
   "outputs": [],
   "source": [
    "label = mnist_train[12][1]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjBzCFXEHmKM"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024 # Reduce this if you get out-of-memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWrWFdYYHmKM"
   },
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNlztloBHmKM"
   },
   "source": [
    "**Create the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao3QwgsgHmKM"
   },
   "source": [
    "**Dropout**:\n",
    "\n",
    "* A regularization technique: i.e. a technique to prevent overfitting, where the model becomes really good on the training dataset, but poor on the validation/testing dataset (unseen data)\n",
    "* Randomly selected neurons are \"switched off\" or prevented from firing during each pass\n",
    "* Thus, at every iteration, the model works with n% of it's neurons missing, where n is specified in the dropout layer (typically 0.5 or 50%)\n",
    "* The result of this is that when random neurons are dropped, the alive neurons have to step in to generate the correct representation to decrease the loss function\n",
    "* Thus the network becomes less sensitive to the weights of a selective few neurons, and become better at generalizing the features without overfitting the training data\n",
    "\n",
    "Paper: [http://jmlr.org/papers/v15/srivastava14a.html](http://jmlr.org/papers/v15/srivastava14a.html)\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ8dfeQuHmKM"
   },
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
    "        # which is what we are using.\n",
    "        \n",
    "        # Convolution Layer 1                             # 28 x 28 x 1  (input)\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
    "        self.relu1 = nn.ReLU()                            # Same as above\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 30, kernel_size=5)     # 20 x 20 x 30  (after 2nd convolution)\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.5)             # Same as above\n",
    "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 30  (after pooling)\n",
    "        self.relu2 = nn.ReLU()                            # Same as above \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(3000, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolution Layer 1                    \n",
    "        x = self.conv1(x)                        \n",
    "        x = self.relu1(x)                        \n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)               \n",
    "        x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Switch from activation maps to vectors\n",
    "        x = x.view(-1, 3000)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQvL2F2HmKM"
   },
   "source": [
    "**Create the objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QruVa7xRHmKM"
   },
   "outputs": [],
   "source": [
    "# The model\n",
    "net = MNISTNet()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJGQu0euHmKM"
   },
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysstIYZ6HmKM"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    net.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    valid_accuracy.append(correct / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    \n",
    "    print ('Epoch %d/%d, Training Loss: %.4f, Training Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LUKf0YXHmKN"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./3.model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7msFomwRHmKN"
   },
   "source": [
    "**Visialize the loss and accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfhrV1o9HmKN"
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(valid_loss, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qeBPLFXHmKN"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_accuracy, label='training accuracy')\n",
    "plt.plot(valid_accuracy, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L27ZJq4HmKN"
   },
   "source": [
    "**Standalone inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x14xxUWMHmKN"
   },
   "outputs": [],
   "source": [
    "image_index = 23\n",
    "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
    "img = Variable(img)\n",
    "label = mnist_valid[image_index][1]\n",
    "\n",
    "net.eval()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    net = net.cpu()\n",
    "    img = img.cpu()\n",
    "    \n",
    "output = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywMHi2nWHmKN"
   },
   "outputs": [],
   "source": [
    "output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfCcB3L4HmKN"
   },
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output.data, 1)\n",
    "print(\"Prediction is: \", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3f-iu0SHmKN"
   },
   "outputs": [],
   "source": [
    "print(\"Actual is is: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpJ2FgWzHmKN"
   },
   "source": [
    "## Homework\n",
    "\n",
    "* Play with the various moving parts in this notebook and try to increase the accuracy\n",
    "* Try a different dataset (http://pytorch.org/docs/master/torchvision/datasets.html)\n",
    "* Try data augmentation (http://pytorch.org/docs/master/torchvision/transforms.html)\n",
    "* Read about contest winning CNN architectures (https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3.cnn.for.digits.classification.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
